{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXYLTLs7rlI"
      },
      "source": [
        "# Neural networks\n",
        "\n",
        "Here we use the E-car dataset on car loan applications.\n",
        "\n",
        "IF you're running this on Google Colab, and only then, should you run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2lWLSol78F8"
      },
      "outputs": [],
      "source": [
        "# !! Run this on Google Colab only.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn01QLMe8HVw"
      },
      "source": [
        "Import the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WiD9xJk8Jfj"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "import sklearn.neighbors\n",
        "import sklearn.datasets\n",
        "import sklearn.neural_network\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivf6xuLs8N6W"
      },
      "source": [
        "Load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNfDlC6w8PW_"
      },
      "outputs": [],
      "source": [
        "DATA_FILEPATH = \"drive/MyDrive/e_car_data.csv\"\n",
        "df = pd.read_csv(DATA_FILEPATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY2wD2c6-rfz"
      },
      "source": [
        "Show summary statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08A2CnnZ-xYH"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZybA9898hwe"
      },
      "source": [
        "Place loans into 8 unordered classes / bins, depending on their acceptance (0-3 for denied, 4-7 for accepted) and their Annual Percentage Rate (APR, into 4 classes each):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Twh5ek8tTj"
      },
      "outputs": [],
      "source": [
        "def loan_to_bin(accept_value, apr_value):\n",
        "  offset = 0\n",
        "  if accept_value:\n",
        "    offset = 4\n",
        "\n",
        "  if apr_value < 4:\n",
        "    return offset\n",
        "\n",
        "  if apr_value < 6:\n",
        "    return 1 + offset\n",
        "\n",
        "  if apr_value < 8:\n",
        "    return 2 + offset\n",
        "\n",
        "  return 3 + offset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLqgrt7v9KXY"
      },
      "source": [
        "Create new labels for the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QkZwo3V93D-"
      },
      "outputs": [],
      "source": [
        "apr = df['apr'].values\n",
        "accept = df['accept'].values\n",
        "\n",
        "y = []\n",
        "for i in range(len(apr)):\n",
        "    accept_value = accept[i]\n",
        "    apr_value = apr[i]\n",
        "    bin = loan_to_bin(accept_value, apr_value)\n",
        "    y.append(bin)\n",
        "\n",
        "# Alternative in one-line with a list comprehension (but less readable):\n",
        "# y = [loan_to_bin(accept[i], apr[i]) for i in range(len(apr))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Inm5AwEWtp"
      },
      "source": [
        "Define the features and preprocess (or scale, or normalize) them, to help with convergence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9QjUY4ERb9"
      },
      "outputs": [],
      "source": [
        "columns = ['tier',\n",
        "           'amount',\n",
        "           'apr',\n",
        "           'prime',\n",
        "           'fico',\n",
        "           'competition apr',\n",
        "           'partner bin']\n",
        "x = df[columns].values\n",
        "\n",
        "# Don't forget to scale features!\n",
        "x_scaled = preprocessing.scale(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvb9gygP-bfF"
      },
      "source": [
        "Split the data into three groups: training, validation, and testing (60/20/20 split). The validation data serves to optimize the algorithm, such as choosing the k in kNN, or choosing the number of neurons in the hidden layer. The testing dataset serves to compare across algorithms.\n",
        "\n",
        "SKLearn has no way to do this in one line, so we do it twice (see [thread on StackOverflow](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWSReyJHEtk8"
      },
      "outputs": [],
      "source": [
        "# Split into training (60%), validation (20%), testing (20%)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=0.25, # 0.25 x 0.8 = 0.2\n",
        "                                                  random_state=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zA25wb5LaMl"
      },
      "source": [
        "# Multi-layer perceptron\n",
        "\n",
        "We'll use a model from SciKit-Learn, which already has all we need (e.g., cross-entropy loss function). We define two hidden layers, with 64 and 32 neurons, and fit it to data. Notice that the accuracy has improved from around 50% with multionimal logistic regression to around 80%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW7tvU76LjSs"
      },
      "outputs": [],
      "source": [
        "mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(64, 32),\n",
        "                                           activation=\"logistic\",\n",
        "                                           max_iter=1000,\n",
        "                                           random_state=42)\n",
        "\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = mlp.predict(x_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of multi-layer perceptron on test data: %.5f\" %\n",
        "      accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq8BLHzqipyZ"
      },
      "source": [
        "# Accuracy comparison of our 3 classifiers\n",
        "\n",
        "Let's compare this to the other two classifiers we have: k-Nearest Neighbors and multinomial logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ9G6P-HipQU"
      },
      "outputs": [],
      "source": [
        "logistic = LogisticRegression(max_iter=int(1e5))\n",
        "logistic.fit(x_train, y_train)\n",
        "y_logistic_pred = logistic.predict(x_test)\n",
        "accuracy_logistic = metrics.accuracy_score(y_test, y_logistic_pred)\n",
        "print(\"Accuracy of logistic regression on test data: %.5f\" %\n",
        "      accuracy_logistic)\n",
        "\n",
        "knn = sklearn.neighbors.KNeighborsClassifier()\n",
        "knn.fit(x_train, y_train)\n",
        "y_knn_pred = knn.predict(x_test)\n",
        "accuracy_knn = metrics.accuracy_score(y_test, y_knn_pred)\n",
        "print(\"Accuracy of kNN regression on test data: %.5f\" %\n",
        "      accuracy_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpAMXiutMOI"
      },
      "source": [
        "# How to choose the number of neighbors / hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSwsta7nteMT"
      },
      "outputs": [],
      "source": [
        "# Define a helper function to be DRY.\n",
        "def fit_knn(k, x_train, y_train, x_test, y_test):\n",
        "  knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(x_train, y_train)\n",
        "  y_knn_pred = knn.predict(x_test)\n",
        "  accuracy = metrics.accuracy_score(y_test, y_knn_pred)\n",
        "  return knn, accuracy\n",
        "\n",
        "\n",
        "# Find the best k.\n",
        "k_range = range(1, 100)\n",
        "accuracy_range = []\n",
        "for k in k_range:\n",
        "  _, accuracy = fit_knn(k, x_train, y_train, x_val, y_val)\n",
        "  accuracy_range.append(accuracy)\n",
        "\n",
        "plt.plot(k_range, accuracy_range)\n",
        "\n",
        "best_k = k_range[np.argmax(accuracy_range)]\n",
        "\n",
        "# Compute accuracy with the best k.\n",
        "_, acc = fit_knn(best_k, x_train, y_train, x_test, y_test)\n",
        "print(\"Accuracy of optimized k-nearest neighbors (k=%d): %.3f\" % (best_k, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pozxYS9fKDf4"
      },
      "source": [
        "# Question\n",
        "\n",
        "Why is the kNN accuracy smaller with k = 13 then before, with default value of k? That is: why did the optimization of hyper-parameters not optimize the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHqpPJ_3mdv6"
      },
      "source": [
        "# Revisiting MNIST hand-written digits\n",
        "\n",
        "In lecture 1 and assignment 2, we used 1-nearest neighbor for MNIST. Here we code it again, using SciKit-Learn, and compare its accuracy to the a neural network. In case you don't have Tensorflow installed, you can use this cell (with a smaller version of the dataset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma-D27Jim9xZ"
      },
      "outputs": [],
      "source": [
        "mnist = sklearn.datasets.load_digits()\n",
        "print(\"Data shape:\", mnist.data.shape)\n",
        "x = mnist.data\n",
        "y = mnist.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,\n",
        "                                                    y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgzoP2n-qaGm"
      },
      "source": [
        "If you have Tensorflow / Keras installed, you can use the full dataset with 60k images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyCxHUqHrBZ6"
      },
      "outputs": [],
      "source": [
        "#!pip install keras\n",
        "import keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print(\"Shape before flattening:\", x_train.shape)\n",
        "\n",
        "# Flatten the data, for input into the Multi-Layer Perceptron.\n",
        "# If we used Tensorflow, we could use a tf.keras.layers.Flatten layer\n",
        "# to convert the image.\n",
        "\n",
        "x_train = x_train.reshape([x_train.shape[0], -1])\n",
        "x_test = x_test.reshape([x_test.shape[0], -1])\n",
        "print(\"Shape after flattening:\", x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pGQ0V23qTgs"
      },
      "source": [
        "Check the output counts, and split the data into training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDWFGoQ2qSOE"
      },
      "outputs": [],
      "source": [
        "print(\"Target classes: \", np.unique(y_train, return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOaYwpj2m_LN"
      },
      "outputs": [],
      "source": [
        "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(x_train, y_train)\n",
        "y_knn_pred = knn.predict(x_test)\n",
        "accuracy_knn = metrics.accuracy_score(y_test, y_knn_pred)\n",
        "print(\"Accuracy of kNN classification on test data: %.5f\" %\n",
        "      accuracy_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: estimate a neural network on MNIST data\n",
        "\n",
        "Complete this next cell."
      ],
      "metadata": {
        "id": "9PpuWQAyDpK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yWNoxZys3eg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# TODO: train a neural network to recognize handwritten digits, and gauge the\n",
        "# accuracy of the model\n",
        "\n",
        "\n",
        "\n",
        "# This line prints how long it took to train the model and estimate accuracy.\n",
        "minutes = (time.time() - start) / 60\n",
        "print(\"Elapsed: %.1f minutes\" % minutes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}