{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZCLZlIEysJW"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Import the required modules. Note that these are for the whole course; some of them may not be required for this specific lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8CUNxcfyuYB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn.linear_model\n",
        "import sklearn.neighbors\n",
        "import sklearn.neural_network\n",
        "\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCXhWHOzthS"
      },
      "source": [
        "Initialize the random seed, for reproducibility, so we all get the same results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmlkEc3SzvsS"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D-NzdG1yXLe"
      },
      "source": [
        "## Data directory and Google Colab\n",
        "\n",
        "I write and run notebooks on Google Colab with data on Google Drive. This cell checks if the notebook is running on Google Colab. If so, it connects to Google Drive. Otherwise, it will look for the data in the current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFllBpV-wV5C"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "RUNNING_ON_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if RUNNING_ON_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  DIR = \"drive/MyDrive\"\n",
        "else:\n",
        "  DIR = \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lxqE84JwZ2R"
      },
      "source": [
        "# Comparison of 3 classifiers with credit card transactions\n",
        "\n",
        "We have learned three classifiers in class:\n",
        "\n",
        "1. k-Nearest Neighbors (with majority voting, i.e. the prediction is the most common class among the k nearest neighbors);\n",
        "\n",
        "2. logistic regression (binomial / binary, or multinomial / multi-class);\n",
        "\n",
        "3. neural networks, or the multi-layer perceptron.\n",
        "\n",
        "Here, we estimate these three classifiers on a dataset of credit card transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAzn0fUAxsI5"
      },
      "source": [
        "## Data\n",
        "\n",
        "We use a publicly available dataset on [credit card transactions](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). For privacy reasons, the explanatory variables (`V1-V28`) in this dataset are obfuscated (they come from \"dimensionality reduction\" via Principal Components Analysis, or PCA, and these are the most relevant factors). The other variables are the transaction amount and the label (1 for fraud, 0 for non-fraud). You need to unzip and extract the dataset in `creditcardfraud.zip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CJvvGGTziId"
      },
      "outputs": [],
      "source": [
        "DATA_FILEPATH = os.path.join(DIR, \"creditcard.csv\")\n",
        "df = pd.read_csv(DATA_FILEPATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8kA0SLTNF_X"
      },
      "source": [
        "## Summary statistics\n",
        "\n",
        "When we get a new dataset, the first thing we should do is look at a few samples of the data. We do this in Pandas with `.head()`, a method on dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNFzhrq7NenY"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwHIaorxN8zm"
      },
      "source": [
        "We see that we have variables called:\n",
        "\n",
        "- `Time` (instead of a timestamp, which could be a privacy risk, this is the time elapsed since the first transaction);\n",
        "\n",
        "- a number of variables, `V1-V28` that look distributed around 0 with a standard deviation around 1;\n",
        "\n",
        "- `Amount`, which looks like a dollar amount in cents;\n",
        "\n",
        "- `Class`, which in this case is full of zeroes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MI1rfgROkaV"
      },
      "source": [
        "The next step is to print summary statistics, with Pandas's dataframe method `.describe()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbgPw_xRNxvi"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zj1409uOpim"
      },
      "source": [
        "We see that:\n",
        "\n",
        "- we have around 300 thousand observations;\n",
        "\n",
        "- the variables `V1-V28` are indeed centered around 0 with a standard deviation close to unity;\n",
        "\n",
        "- the average purchase amount is 88 dollars, ranging from 0 (for example, for a card verification in a free trial, that does not charge money at the start) to 26 thousand dollars (for example, to buy a car)\n",
        "\n",
        "- `Class`, which is the flag for a fraudulent transaction, is mostly zeroes, with 0.17% of transactions that are fraudulent.\n",
        "\n",
        "This is called \"class imbalance\": one class (normal transactions) covers most of the dataset. This will become a problem: a prediction function that returns 0 in all cases will on average achieve 99.83% accuracy (100% - 0.17%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Vt5A9b_cec"
      },
      "source": [
        "## Plot the data\n",
        "\n",
        "The second thing to do with a new dataset is to plot the data. Here, we plot two overlaying histograms of each of the 28 variables, once for fraudulent transactions, and again for legitimate transactions. We scale the histograms so that we see the distribution despite the class imbalance.\n",
        "\n",
        "We access the rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAw0CSCM_cJD"
      },
      "outputs": [],
      "source": [
        "y = df[\"Class\"]\n",
        "\n",
        "legitimate = df[y == 0]\n",
        "fraudulent = df[y == 1]\n",
        "\n",
        "for num in range(1, 29):\n",
        "  variable = f\"V{num:d}\"\n",
        "\n",
        "  min_x = min(df[variable])\n",
        "  max_x = max(df[variable])\n",
        "\n",
        "  bins = np.linspace(min_x, max_x, 100)\n",
        "\n",
        "  plt.hist(legitimate[variable], bins, alpha=0.5, label=\"legit\", density=True)\n",
        "  plt.hist(fraudulent[variable], bins, alpha=0.5, label=\"fraud\", density=True)\n",
        "  plt.title(variable)\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFpThYb3CCtt"
      },
      "source": [
        "Some of these have clearly different distributions, so we should find a model with reasonable accuracy in distinguishing legitimate and fraudulent transactions.\n",
        "\n",
        "We also see that some distributions have wide stretches, i.e. there are outliers. For example, for `V28`, the values are centered around 0 with 1-2 for standard deviation, but the maximum value is over 30.\n",
        "\n",
        "In a real-world application, you may want to deal with these outliers first, for example:\n",
        "\n",
        "- discarding outliers;\n",
        "\n",
        "- \"winsorizing\" extreme values (so the maximum value of 30 gets replaced with the value at the 95% quantile, for example);\n",
        "\n",
        "- choosing a model that is not sensitive to outliers.\n",
        "\n",
        "In this course, for simplicity, we ignore them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB7uQFjeq8Ni"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "We have already trained a neural network before, i.e. we found the optimal parameters (weights and biases) to minimize the cross-entropy loss.\n",
        "\n",
        "There are other parameters we can optimize to improve the model: the number of layers, the number of neurons in each layer, the activation function in each layer, etc. To distinguish these from the usual term \"parameters\" in statistics, we call these \"hyperparameters\".\n",
        "\n",
        "## Hyperparameter optimization\n",
        "\n",
        "These parameters can be optimized, similar to parameters. For example, take the activation function. A model using logistic/sigmoid is different from a model using ReLU. We choose the best between the two by comparing some criterion (cross-entropy loss, or accuracy) on data that neither model has seen before.\n",
        "\n",
        "It is similar to kNN and the choice of k. In the kNN example, we split a dataset into training and testing. The training dataset serves to \"train the model\" (even though kNN has no training). The test dataset served to optimize the value of k using the accuracy of the model on data that the model has not seen (otherwise, it would be similar to giving an exam with exercises from assignments or lectures: students could just repeat what they already saw).\n",
        "\n",
        "The dataset that serves for hyperparameter optimization is called \"validation dataset\".\n",
        "\n",
        "The procedure is then:\n",
        "\n",
        "- one or more for loops on the hyperparameter values we want to optimize;\n",
        "\n",
        "- for each possible combination, train the model on the training dataset;\n",
        "\n",
        "- compute a criterion (cross-entropy loss or accuracy) of the trained model on a test dataset;\n",
        "\n",
        "- choose the value of the hyperparameters that optimizes that criterion.\n",
        "\n",
        "## Train-validation-test split\n",
        "\n",
        "We can use the training and validation datasets to optimize one particular model, such as neural networks. To compare across models, such as neural networks versus kNN, we need another dataset, that no model has seen before. This is typically called the \"test dataset\".\n",
        "\n",
        "So our original dataset gets split into 3 datasets, called  a \"train-validation-test\" split:\n",
        "\n",
        "- the training data serves to \"train the model\", e.g. to train logistic regression, kNN or a neural network;\n",
        "\n",
        "- the validation dataset serves for hyperparameter optimization: finding the best value of k (in the case of kNN), or to find the best activation function, or number of layers, or number of neurons in each layer (in the case of neural networks);\n",
        "\n",
        "- the test dataset serves to compare the accuracy across models and choose the best one, on data that no model has seen before.\n",
        "\n",
        "Since logistic regression does not allow hyperparameter optimization, we can actually estimate it on the combination of training and validation datasets.\n",
        "\n",
        "## Train-test-validation split in SKLearn\n",
        "\n",
        "SciKit-Learn has no possibility to split a dataset into three parts, so we split into two parts twice. We want a split 60-20-20, so we split first at 20%, then at 25% (because 25% of (100 - 80) = 0.25 * 0.8 = 20%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHd3TWWM_OO5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df[\"Class\"]\n",
        "\n",
        "variables = list([f\"V{num:d}\" for d in range(1, 29)])\n",
        "X = df[variables]\n",
        "\n",
        "train_test_split(X, y, random_state=0, test_size=0.7)\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X,\n",
        "                                                            y,\n",
        "                                                            random_state=0,\n",
        "                                                            test_size=0.2)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val,\n",
        "                                                  y_train_val,\n",
        "                                                  random_state=0,\n",
        "                                                  test_size=0.25)\n",
        "\n",
        "print(f\"{X_train.shape=}\")\n",
        "print(f\"{X_val.shape=}\")\n",
        "print(f\"{X_test.shape=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPQZRtkXEi2b"
      },
      "source": [
        "## Logistic regression / classification\n",
        "\n",
        "Let's estimate a logistic regression on the training + validation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKBvyr7DEros"
      },
      "outputs": [],
      "source": [
        "logistic_classifier = sklearn.linear_model.LogisticRegression(max_iter=int(1e5))\n",
        "logistic_classifier.fit(X_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ0Bw0qjE7tI"
      },
      "source": [
        "## k-Nearest Neighbors\n",
        "\n",
        "Let's optimize the k-Nearest Neighbors classifier for k between, say, 1 and 10 thousand (10 thousand so that the classifier has a chance to find some fraudulent transactions, on average, 17 of them, since the proportion of fraudulent transactions is 0.17%).\n",
        "\n",
        "This would take a very long time: for one value of k, the prediction on the test set compares all elements in the test set to all elements in the training set, so around 10 billion (170,883 * 56,962 = 9,733,837,446) computation of Euclidean distance in 28 dimensions.\n",
        "\n",
        "Even a logarithmic scale, 10, 100, 1000, takes about one hour. 10 thousand fails after using all available RAM (Random-Access Memory).\n",
        "\n",
        "So let's only do two values of k: 100 and 1 thousand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3r-wznb7Na4J"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0\n",
        "best_k = 0\n",
        "best_knn = None\n",
        "\n",
        "for k in [100, 1000]:\n",
        "\n",
        "  print(f\"Trying {k=}... \", end=\"\")\n",
        "  knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  print(\"fit! Computing accuracy... \", end=\"\")\n",
        "  accuracy = knn.score(X_val, y_test)\n",
        "\n",
        "  print(f\"computed! {accuracy=:.6f}\")\n",
        "  if accuracy > best_accuracy:\n",
        "    best_k = k\n",
        "    best_knn = knn\n",
        "    best_accuracy = accuracy\n",
        "\n",
        "print(f\"The optimal value of k is {best_k} with accuracy {best_accuracy:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxCYoAy7HfpW"
      },
      "source": [
        "## Neural networks\n",
        "\n",
        "We could try a neural network with between 1 and 3 layers, with between 10 and 20 neurons each, and with activation functions either logistic or ReLU. But that would also take too long. So instead, let's try 2-3 hidden layers, with a number of neurons either 10, 15 or 20, but strictly decreasing from one layer to the next. The list of possible hidden layer sizes is then:\n",
        "\n",
        "```\n",
        "[(20, 15), (20, 10), (15, 10), (20, 15, 10)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QORKeFO9L1th"
      },
      "source": [
        "Next, we optimize over number of layers, neurons in each layer, and activation function to find the best fit of a Multi-Layer Perceptron:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uau8pxo0MAL7"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0\n",
        "best_hidden_layers = None\n",
        "best_mlp = None\n",
        "best_activation = None\n",
        "\n",
        "for combination in [(20, 15), (20, 10), (15, 10), (20, 15, 10)]:\n",
        "  for activation in [\"logistic\", \"relu\"]:\n",
        "\n",
        "    print(f\"Fitting {combination=}, {activation=}... \", end=\"\")\n",
        "    mlp = sklearn.neural_network.MLPClassifier(\n",
        "      hidden_layer_sizes=combination,\n",
        "      max_iter=2000,\n",
        "      random_state=42,\n",
        "      activation=activation\n",
        "    )\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Fit! Accuracy... \", end=\"\")\n",
        "    y_predicted = mlp.predict(X_val)\n",
        "    accuracy = sklearn.metrics.accuracy_score(y_predicted, y_val)\n",
        "    # Alternatively:\n",
        "    # accuracy = mlp.score(X_val, y_val)\n",
        "\n",
        "    print(f\"computed! {accuracy=:.6f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "      best_accuracy = accuracy\n",
        "      best_hidden_layers = combination\n",
        "      best_mlp = mlp\n",
        "      best_activation = activation\n",
        "\n",
        "print(f\"Found best MLP model: {best_hidden_layers=}, {best_activation=}\")\n",
        "print(f\"{best_accuracy=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9qlxherNwrC"
      },
      "source": [
        "## Comparison of the three classifiers\n",
        "\n",
        "Now that we have estimated and fit the three classifiers, let's compare their performance on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JRAMv0TsN5iK"
      },
      "outputs": [],
      "source": [
        "acc_test_logistic = logistic_classifier.score(X_test, y_test)\n",
        "\n",
        "acc_test_knn = best_knn.score(X_test, y_test)\n",
        "\n",
        "acc_test_mlp = best_mlp.score(X_test, y_test)\n",
        "\n",
        "print(f\"{acc_test_logistic=}, {acc_test_knn=}, {acc_test_mlp=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BZqFzUKOgCZ"
      },
      "source": [
        "Note: it's possible to make this code DRY with the function `eval()`, which converts a string into code and evaluates that code. The following cell does the same as the previous, but is shorter, and more complicated (and also uses the paradigm of \"functions as first class citizens\", storing a function in a variable, and using it after). Try this only if you're comfortable with the material so far. It will not be in the exam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waZfPkFMOv-H"
      },
      "outputs": [],
      "source": [
        "for model_string in [\"logistic_classifier\", \"best_knn\", \"best_mlp\"]:\n",
        "  model_function = eval(model_string)\n",
        "  accuracy = model_function.score(X_test, y_test)\n",
        "  print(f\"For model {model_string}, {accuracy=:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkBz6wiwQZRq"
      },
      "source": [
        "If you get different results, please come talk to me, so I can understand and debug the discrepancy.\n",
        "\n",
        "The accuracy scores are similar for all models, due to the class imbalance (read more [here](https://stackoverflow.com/questions/67878862/why-are-all-my-classification-accuracy-scores-the-same))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vavkE4F5Pijz"
      },
      "source": [
        "## Situation specific to the class imbalance\n",
        "\n",
        "Without the class imbalance, the procedure ends here: we choose the best model from a criterion of accuracy.\n",
        "\n",
        "When the data has a class imbalance like this credit card dataset, we may need more. For example, a naive model that always predict a legitimate transaction achieves 99.83% accuracy.\n",
        "\n",
        "We could pursue the analysis with other metrics, for example the F-1 score, plotting a confusion matrix, or the Area Under the Precision-Recall Curve (AUPRC)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}